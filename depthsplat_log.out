/work/courses/3dv/35/envs/depthsplat/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("xformers_flash::flash_fwd")
/work/courses/3dv/35/envs/depthsplat/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("xformers_flash::flash_bwd")
[36mSaving outputs to checkpoints/re10k-256x256-depthsplat-small.[39m
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
Using cache found in /home/maqima/.cache/torch/hub/facebookresearch_dinov2_main
[2025-05-31 20:07:04,104][dinov2][INFO] - using MLP layer as FFN
Using cache found in /home/maqima/.cache/torch/hub/verlab_accelerated_features_main
wandb: Currently logged in as: seamoon2020 (seamoon2020-eth-z-rich). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.19.11 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.7
wandb: Run data is saved locally in checkpoints/re10k-256x256-depthsplat-small/wandb/run-20250531_200712-zikte9en
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run re10k-256x256-depthsplat-small
wandb: ‚≠êÔ∏è View project at https://wandb.ai/seamoon2020-eth-z-rich/depthsplat
wandb: üöÄ View run at https://wandb.ai/seamoon2020-eth-z-rich/depthsplat/runs/zikte9en
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name       | Type                 | Params | Mode 
------------------------------------------------------------
0 | encoder    | EncoderDepthSplat    | 38.4 M | train
1 | decoder    | DecoderSplattingCUDA | 0      | train
2 | losses     | ModuleList           | 0      | train
3 | roma_model | TinyRoMa             | 2.8 M  | train
------------------------------------------------------------
18.1 M    Trainable params
23.1 M    Non-trainable params
41.2 M    Total params
164.794   Total estimated model params size (MB)
786       Modules in train mode
161       Modules in eval mode
Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]
Loading model from: /work/courses/3dv/35/envs/depthsplat/lib/python3.10/site-packages/lpips/weights/v0.1/vgg.pth
train: 1308
val: 1
test: 7286
Froze: encoder.depth_predictor.backbone.conv1.weight
Froze: encoder.depth_predictor.backbone.layer1.0.conv1.weight
Froze: encoder.depth_predictor.backbone.layer1.0.conv2.weight
Froze: encoder.depth_predictor.backbone.layer1.1.conv1.weight
Froze: encoder.depth_predictor.backbone.layer1.1.conv2.weight
Froze: encoder.depth_predictor.backbone.layer2.0.conv1.weight
Froze: encoder.depth_predictor.backbone.layer2.0.conv2.weight
Froze: encoder.depth_predictor.backbone.layer2.0.downsample.0.weight
Froze: encoder.depth_predictor.backbone.layer2.0.downsample.0.bias
Froze: encoder.depth_predictor.backbone.layer2.1.conv1.weight
Froze: encoder.depth_predictor.backbone.layer2.1.conv2.weight
Froze: encoder.depth_predictor.backbone.layer3.0.conv1.weight
Froze: encoder.depth_predictor.backbone.layer3.0.conv2.weight
Froze: encoder.depth_predictor.backbone.layer3.0.downsample.0.weight
Froze: encoder.depth_predictor.backbone.layer3.0.downsample.0.bias
Froze: encoder.depth_predictor.backbone.layer3.1.conv1.weight
Froze: encoder.depth_predictor.backbone.layer3.1.conv2.weight
Froze: encoder.depth_predictor.backbone.conv2.weight
Froze: encoder.depth_predictor.backbone.conv2.bias
Froze: encoder.depth_predictor.pretrained.cls_token
Froze: encoder.depth_predictor.pretrained.pos_embed
Froze: encoder.depth_predictor.pretrained.patch_embed.proj.weight
Froze: encoder.depth_predictor.pretrained.patch_embed.proj.bias
Froze: encoder.depth_predictor.pretrained.blocks.0.norm1.weight
Froze: encoder.depth_predictor.pretrained.blocks.0.norm1.bias
Froze: encoder.depth_predictor.pretrained.blocks.0.attn.qkv.weight
Froze: encoder.depth_predictor.pretrained.blocks.0.attn.qkv.bias
Froze: encoder.depth_predictor.pretrained.blocks.0.attn.proj.weight
Froze: encoder.depth_predictor.pretrained.blocks.0.attn.proj.bias
Froze: encoder.depth_predictor.pretrained.blocks.0.ls1.gamma
Froze: encoder.depth_predictor.pretrained.blocks.0.norm2.weight
Froze: encoder.depth_predictor.pretrained.blocks.0.norm2.bias
Froze: encoder.depth_predictor.pretrained.blocks.0.mlp.fc1.weight
Froze: encoder.depth_predictor.pretrained.blocks.0.mlp.fc1.bias
Froze: encoder.depth_predictor.pretrained.blocks.0.mlp.fc2.weight
Froze: encoder.depth_predictor.pretrained.blocks.0.mlp.fc2.bias
Froze: encoder.depth_predictor.pretrained.blocks.0.ls2.gamma
Froze: encoder.depth_predictor.pretrained.blocks.1.norm1.weight
Froze: encoder.depth_predictor.pretrained.blocks.1.norm1.bias
Froze: encoder.depth_predictor.pretrained.blocks.1.attn.qkv.weight
Froze: encoder.depth_predictor.pretrained.blocks.1.attn.qkv.bias
Froze: encoder.depth_predictor.pretrained.blocks.1.attn.proj.weight
Froze: encoder.depth_predictor.pretrained.blocks.1.attn.proj.bias
Froze: encoder.depth_predictor.pretrained.blocks.1.ls1.gamma
Froze: encoder.depth_predictor.pretrained.blocks.1.norm2.weight
Froze: encoder.depth_predictor.pretrained.blocks.1.norm2.bias
Froze: encoder.depth_predictor.pretrained.blocks.1.mlp.fc1.weight
Froze: encoder.depth_predictor.pretrained.blocks.1.mlp.fc1.bias
Froze: encoder.depth_predictor.pretrained.blocks.1.mlp.fc2.weight
Froze: encoder.depth_predictor.pretrained.blocks.1.mlp.fc2.bias
Froze: encoder.depth_predictor.pretrained.blocks.1.ls2.gamma
Froze: encoder.depth_predictor.pretrained.blocks.2.norm1.weight
Froze: encoder.depth_predictor.pretrained.blocks.2.norm1.bias
Froze: encoder.depth_predictor.pretrained.blocks.2.attn.qkv.weight
Froze: encoder.depth_predictor.pretrained.blocks.2.attn.qkv.bias
Froze: encoder.depth_predictor.pretrained.blocks.2.attn.proj.weight
Froze: encoder.depth_predictor.pretrained.blocks.2.attn.proj.bias
Froze: encoder.depth_predictor.pretrained.blocks.2.ls1.gamma
Froze: encoder.depth_predictor.pretrained.blocks.2.norm2.weight
Froze: encoder.depth_predictor.pretrained.blocks.2.norm2.bias
Froze: encoder.depth_predictor.pretrained.blocks.2.mlp.fc1.weight
Froze: encoder.depth_predictor.pretrained.blocks.2.mlp.fc1.bias
Froze: encoder.depth_predictor.pretrained.blocks.2.mlp.fc2.weight
Froze: encoder.depth_predictor.pretrained.blocks.2.mlp.fc2.bias
Froze: encoder.depth_predictor.pretrained.blocks.2.ls2.gamma
Froze: encoder.depth_predictor.pretrained.blocks.3.norm1.weight
Froze: encoder.depth_predictor.pretrained.blocks.3.norm1.bias
Froze: encoder.depth_predictor.pretrained.blocks.3.attn.qkv.weight
Froze: encoder.depth_predictor.pretrained.blocks.3.attn.qkv.bias
Froze: encoder.depth_predictor.pretrained.blocks.3.attn.proj.weight
Froze: encoder.depth_predictor.pretrained.blocks.3.attn.proj.bias
Froze: encoder.depth_predictor.pretrained.blocks.3.ls1.gamma
Froze: encoder.depth_predictor.pretrained.blocks.3.norm2.weight
Froze: encoder.depth_predictor.pretrained.blocks.3.norm2.bias
Froze: encoder.depth_predictor.pretrained.blocks.3.mlp.fc1.weight
Froze: encoder.depth_predictor.pretrained.blocks.3.mlp.fc1.bias
Froze: encoder.depth_predictor.pretrained.blocks.3.mlp.fc2.weight
Froze: encoder.depth_predictor.pretrained.blocks.3.mlp.fc2.bias
Froze: encoder.depth_predictor.pretrained.blocks.3.ls2.gamma
Froze: encoder.depth_predictor.pretrained.blocks.4.norm1.weight
Froze: encoder.depth_predictor.pretrained.blocks.4.norm1.bias
Froze: encoder.depth_predictor.pretrained.blocks.4.attn.qkv.weight
Froze: encoder.depth_predictor.pretrained.blocks.4.attn.qkv.bias
Froze: encoder.depth_predictor.pretrained.blocks.4.attn.proj.weight
Froze: encoder.depth_predictor.pretrained.blocks.4.attn.proj.bias
Froze: encoder.depth_predictor.pretrained.blocks.4.ls1.gamma
Froze: encoder.depth_predictor.pretrained.blocks.4.norm2.weight
Froze: encoder.depth_predictor.pretrained.blocks.4.norm2.bias
Froze: encoder.depth_predictor.pretrained.blocks.4.mlp.fc1.weight
Froze: encoder.depth_predictor.pretrained.blocks.4.mlp.fc1.bias
Froze: encoder.depth_predictor.pretrained.blocks.4.mlp.fc2.weight
Froze: encoder.depth_predictor.pretrained.blocks.4.mlp.fc2.bias
Froze: encoder.depth_predictor.pretrained.blocks.4.ls2.gamma
Froze: encoder.depth_predictor.pretrained.blocks.5.norm1.weight
Froze: encoder.depth_predictor.pretrained.blocks.5.norm1.bias
Froze: encoder.depth_predictor.pretrained.blocks.5.attn.qkv.weight
Froze: encoder.depth_predictor.pretrained.blocks.5.attn.qkv.bias
Froze: encoder.depth_predictor.pretrained.blocks.5.attn.proj.weight
Froze: encoder.depth_predictor.pretrained.blocks.5.attn.proj.bias
Froze: encoder.depth_predictor.pretrained.blocks.5.ls1.gamma
Froze: encoder.depth_predictor.pretrained.blocks.5.norm2.weight
Froze: encoder.depth_predictor.pretrained.blocks.5.norm2.bias
Froze: encoder.depth_predictor.pretrained.blocks.5.mlp.fc1.weight
Froze: encoder.depth_predictor.pretrained.blocks.5.mlp.fc1.bias
Froze: encoder.depth_predictor.pretrained.blocks.5.mlp.fc2.weight
Froze: encoder.depth_predictor.pretrained.blocks.5.mlp.fc2.bias
Froze: encoder.depth_predictor.pretrained.blocks.5.ls2.gamma
Froze: encoder.depth_predictor.pretrained.blocks.6.norm1.weight
Froze: encoder.depth_predictor.pretrained.blocks.6.norm1.bias
Froze: encoder.depth_predictor.pretrained.blocks.6.attn.qkv.weight
Froze: encoder.depth_predictor.pretrained.blocks.6.attn.qkv.bias
Froze: encoder.depth_predictor.pretrained.blocks.6.attn.proj.weight
Froze: encoder.depth_predictor.pretrained.blocks.6.attn.proj.bias
Froze: encoder.depth_predictor.pretrained.blocks.6.ls1.gamma
Froze: encoder.depth_predictor.pretrained.blocks.6.norm2.weight
Froze: encoder.depth_predictor.pretrained.blocks.6.norm2.bias
Froze: encoder.depth_predictor.pretrained.blocks.6.mlp.fc1.weight
Froze: encoder.depth_predictor.pretrained.blocks.6.mlp.fc1.bias
Froze: encoder.depth_predictor.pretrained.blocks.6.mlp.fc2.weight
Froze: encoder.depth_predictor.pretrained.blocks.6.mlp.fc2.bias
Froze: encoder.depth_predictor.pretrained.blocks.6.ls2.gamma
Froze: encoder.depth_predictor.pretrained.blocks.7.norm1.weight
Froze: encoder.depth_predictor.pretrained.blocks.7.norm1.bias
Froze: encoder.depth_predictor.pretrained.blocks.7.attn.qkv.weight
Froze: encoder.depth_predictor.pretrained.blocks.7.attn.qkv.bias
Froze: encoder.depth_predictor.pretrained.blocks.7.attn.proj.weight
Froze: encoder.depth_predictor.pretrained.blocks.7.attn.proj.bias
Froze: encoder.depth_predictor.pretrained.blocks.7.ls1.gamma
Froze: encoder.depth_predictor.pretrained.blocks.7.norm2.weight
Froze: encoder.depth_predictor.pretrained.blocks.7.norm2.bias
Froze: encoder.depth_predictor.pretrained.blocks.7.mlp.fc1.weight
Froze: encoder.depth_predictor.pretrained.blocks.7.mlp.fc1.bias
Froze: encoder.depth_predictor.pretrained.blocks.7.mlp.fc2.weight
Froze: encoder.depth_predictor.pretrained.blocks.7.mlp.fc2.bias
Froze: encoder.depth_predictor.pretrained.blocks.7.ls2.gamma
Froze: encoder.depth_predictor.pretrained.blocks.8.norm1.weight
Froze: encoder.depth_predictor.pretrained.blocks.8.norm1.bias
Froze: encoder.depth_predictor.pretrained.blocks.8.attn.qkv.weight
Froze: encoder.depth_predictor.pretrained.blocks.8.attn.qkv.bias
Froze: encoder.depth_predictor.pretrained.blocks.8.attn.proj.weight
Froze: encoder.depth_predictor.pretrained.blocks.8.attn.proj.bias
Froze: encoder.depth_predictor.pretrained.blocks.8.ls1.gamma
Froze: encoder.depth_predictor.pretrained.blocks.8.norm2.weight
Froze: encoder.depth_predictor.pretrained.blocks.8.norm2.bias
Froze: encoder.depth_predictor.pretrained.blocks.8.mlp.fc1.weight
Froze: encoder.depth_predictor.pretrained.blocks.8.mlp.fc1.bias
Froze: encoder.depth_predictor.pretrained.blocks.8.mlp.fc2.weight
Froze: encoder.depth_predictor.pretrained.blocks.8.mlp.fc2.bias
Froze: encoder.depth_predictor.pretrained.blocks.8.ls2.gamma
Froze: encoder.depth_predictor.pretrained.blocks.9.norm1.weight
Froze: encoder.depth_predictor.pretrained.blocks.9.norm1.bias
Froze: encoder.depth_predictor.pretrained.blocks.9.attn.qkv.weight
Froze: encoder.depth_predictor.pretrained.blocks.9.attn.qkv.bias
Froze: encoder.depth_predictor.pretrained.blocks.9.attn.proj.weight
Froze: encoder.depth_predictor.pretrained.blocks.9.attn.proj.bias
Froze: encoder.depth_predictor.pretrained.blocks.9.ls1.gamma
Froze: encoder.depth_predictor.pretrained.blocks.9.norm2.weight
Froze: encoder.depth_predictor.pretrained.blocks.9.norm2.bias
Froze: encoder.depth_predictor.pretrained.blocks.9.mlp.fc1.weight
Froze: encoder.depth_predictor.pretrained.blocks.9.mlp.fc1.bias
Froze: encoder.depth_predictor.pretrained.blocks.9.mlp.fc2.weight
Froze: encoder.depth_predictor.pretrained.blocks.9.mlp.fc2.bias
Froze: encoder.depth_predictor.pretrained.blocks.9.ls2.gamma
Froze: encoder.depth_predictor.pretrained.blocks.10.norm1.weight
Froze: encoder.depth_predictor.pretrained.blocks.10.norm1.bias
Froze: encoder.depth_predictor.pretrained.blocks.10.attn.qkv.weight
Froze: encoder.depth_predictor.pretrained.blocks.10.attn.qkv.bias
Froze: encoder.depth_predictor.pretrained.blocks.10.attn.proj.weight
Froze: encoder.depth_predictor.pretrained.blocks.10.attn.proj.bias
Froze: encoder.depth_predictor.pretrained.blocks.10.ls1.gamma
Froze: encoder.depth_predictor.pretrained.blocks.10.norm2.weight
Froze: encoder.depth_predictor.pretrained.blocks.10.norm2.bias
Froze: encoder.depth_predictor.pretrained.blocks.10.mlp.fc1.weight
Froze: encoder.depth_predictor.pretrained.blocks.10.mlp.fc1.bias
Froze: encoder.depth_predictor.pretrained.blocks.10.mlp.fc2.weight
Froze: encoder.depth_predictor.pretrained.blocks.10.mlp.fc2.bias
Froze: encoder.depth_predictor.pretrained.blocks.10.ls2.gamma
Froze: encoder.depth_predictor.pretrained.blocks.11.norm1.weight
Froze: encoder.depth_predictor.pretrained.blocks.11.norm1.bias
Froze: encoder.depth_predictor.pretrained.blocks.11.attn.qkv.weight
Froze: encoder.depth_predictor.pretrained.blocks.11.attn.qkv.bias
Froze: encoder.depth_predictor.pretrained.blocks.11.attn.proj.weight
Froze: encoder.depth_predictor.pretrained.blocks.11.attn.proj.bias
Froze: encoder.depth_predictor.pretrained.blocks.11.ls1.gamma
Froze: encoder.depth_predictor.pretrained.blocks.11.norm2.weight
Froze: encoder.depth_predictor.pretrained.blocks.11.norm2.bias
Froze: encoder.depth_predictor.pretrained.blocks.11.mlp.fc1.weight
Froze: encoder.depth_predictor.pretrained.blocks.11.mlp.fc1.bias
Froze: encoder.depth_predictor.pretrained.blocks.11.mlp.fc2.weight
Froze: encoder.depth_predictor.pretrained.blocks.11.mlp.fc2.bias
Froze: encoder.depth_predictor.pretrained.blocks.11.ls2.gamma
Froze: encoder.depth_predictor.pretrained.norm.weight
Froze: encoder.depth_predictor.pretrained.norm.bias
[36mLoaded pretrained weights: pretrained/depthsplat-gs-small-re10k-256x256-view2-cfeab6b1.pth[39m
validation step 0; scene = ['306e2b7785657539']; context = [[14, 44]]
Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]
Loading model from: /work/courses/3dv/35/envs/depthsplat/lib/python3.10/site-packages/lpips/weights/v0.1/vgg.pth
EncoderDepthSplat(
  (depth_predictor): MultiViewUniMatch(
    (backbone): CNNEncoder(
      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (norm1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      (relu1): ReLU(inplace=True)
      (layer1): Sequential(
        (0): ResidualBlock(
          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (relu): ReLU(inplace=True)
          (norm1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          (norm2): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        )
        (1): ResidualBlock(
          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (relu): ReLU(inplace=True)
          (norm1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          (norm2): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        )
      )
      (layer2): Sequential(
        (0): ResidualBlock(
          (conv1): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (relu): ReLU(inplace=True)
          (norm1): InstanceNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          (norm2): InstanceNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          (norm3): InstanceNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          (downsample): Sequential(
            (0): Conv2d(64, 96, kernel_size=(1, 1), stride=(1, 1))
            (1): InstanceNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (1): ResidualBlock(
          (conv1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (relu): ReLU(inplace=True)
          (norm1): InstanceNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          (norm2): InstanceNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        )
      )
      (layer3): Sequential(
        (0): ResidualBlock(
          (conv1): Conv2d(96, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (relu): ReLU(inplace=True)
          (norm1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          (norm2): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          (norm3): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          (downsample): Sequential(
            (0): Conv2d(96, 128, kernel_size=(1, 1), stride=(2, 2))
            (1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (1): ResidualBlock(
          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (relu): ReLU(inplace=True)
          (norm1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          (norm2): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        )
      )
      (conv2): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))
    )
    (transformer): MultiViewFeatureTransformer(
      (layers): ModuleList(
        (0-5): 6 x TransformerBlock(
          (self_attn): TransformerLayer(
            (q_proj): Linear(in_features=128, out_features=128, bias=False)
            (k_proj): Linear(in_features=128, out_features=128, bias=False)
            (v_proj): Linear(in_features=128, out_features=128, bias=False)
            (merge): Linear(in_features=128, out_features=128, bias=False)
            (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          )
          (cross_attn_ffn): TransformerLayer(
            (q_proj): Linear(in_features=128, out_features=128, bias=False)
            (k_proj): Linear(in_features=128, out_features=128, bias=False)
            (v_proj): Linear(in_features=128, out_features=128, bias=False)
            (merge): Linear(in_features=128, out_features=128, bias=False)
            (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (0): Linear(in_features=256, out_features=1024, bias=False)
              (1): GELU(approximate='none')
              (2): Linear(in_features=1024, out_features=128, bias=False)
            )
            (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
    )
    (pretrained): DinoVisionTransformer(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 384, kernel_size=(14, 14), stride=(14, 14))
        (norm): Identity()
      )
      (blocks): ModuleList(
        (0-11): 12 x NestedTensorBlock(
          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (attn): MemEffAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): LayerScale()
          (drop_path1): Identity()
          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (ls2): LayerScale()
          (drop_path2): Identity()
        )
      )
      (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (head): Identity()
    )
    (regressor): ModuleList(
      (0): Sequential(
        (0): Conv2d(768, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): GroupNorm(8, 128, eps=1e-05, affine=True)
        (2): GELU(approximate='none')
        (3): UNetModel(
          (input_blocks): ModuleList(
            (0): Sequential(
              (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
            (1): Sequential(
              (0): ResBlock(
                (in_layers): Sequential(
                  (0): GroupNorm8(8, 128, eps=1e-05, affine=True)
                  (1): SiLU()
                  (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                )
                (h_upd): Identity()
                (x_upd): Identity()
                (out_layers): Sequential(
                  (0): GroupNorm8(8, 128, eps=1e-05, affine=True)
                  (1): SiLU()
                  (2): Dropout(p=0, inplace=False)
                  (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                )
                (skip_connection): Identity()
              )
            )
            (2): Sequential(
              (0): Downsample(
                (op): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
              )
            )
            (3): Sequential(
              (0): ResBlock(
                (in_layers): Sequential(
                  (0): GroupNorm8(8, 128, eps=1e-05, affine=True)
                  (1): SiLU()
                  (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                )
                (h_upd): Identity()
                (x_upd): Identity()
                (out_layers): Sequential(
                  (0): GroupNorm8(8, 128, eps=1e-05, affine=True)
                  (1): SiLU()
                  (2): Dropout(p=0, inplace=False)
                  (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                )
                (skip_connection): Identity()
              )
            )
            (4): Sequential(
              (0): Downsample(
                (op): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
              )
            )
            (5): Sequential(
              (0): ResBlock(
                (in_layers): Sequential(
                  (0): GroupNorm8(8, 128, eps=1e-05, affine=True)
                  (1): SiLU()
                  (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                )
                (h_upd): Identity()
                (x_upd): Identity()
                (out_layers): Sequential(
                  (0): GroupNorm8(8, 128, eps=1e-05, affine=True)
                  (1): SiLU()
                  (2): Dropout(p=0, inplace=False)
                  (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                )
                (skip_connection): Identity()
              )
              (1): AttentionBlock(
                (qkv): Conv1d(128, 384, kernel_size=(1,), stride=(1,))
                (attention): QKVAttentionLegacy()
                (norm): GroupNorm8(8, 128, eps=1e-05, affine=True)
                (proj_out): Conv1d(128, 128, kernel_size=(1,), stride=(1,))
              )
            )
          )
          (middle_block): Sequential(
            (0): ResBlock(
              (in_layers): Sequential(
                (0): GroupNorm8(8, 128, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
              (h_upd): Identity()
              (x_upd): Identity()
              (out_layers): Sequential(
                (0): GroupNorm8(8, 128, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0, inplace=False)
                (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
              (skip_connection): Identity()
            )
            (1): Identity()
            (2): ResBlock(
              (in_layers): Sequential(
                (0): GroupNorm8(8, 128, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
              (h_upd): Identity()
              (x_upd): Identity()
              (out_layers): Sequential(
                (0): GroupNorm8(8, 128, eps=1e-05, affine=True)
                (1): SiLU()
                (2): Dropout(p=0, inplace=False)
                (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
              (skip_connection): Identity()
            )
          )
          (output_blocks): ModuleList(
            (0): Sequential(
              (0): ResBlock(
                (in_layers): Sequential(
                  (0): GroupNorm8(8, 256, eps=1e-05, affine=True)
                  (1): SiLU()
                  (2): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                )
                (h_upd): Identity()
                (x_upd): Identity()
                (out_layers): Sequential(
                  (0): GroupNorm8(8, 128, eps=1e-05, affine=True)
                  (1): SiLU()
                  (2): Dropout(p=0, inplace=False)
                  (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                )
                (skip_connection): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
              )
              (1): AttentionBlock(
                (qkv): Conv1d(128, 384, kernel_size=(1,), stride=(1,))
                (attention): QKVAttentionLegacy()
                (norm): GroupNorm8(8, 128, eps=1e-05, affine=True)
                (proj_out): Conv1d(128, 128, kernel_size=(1,), stride=(1,))
              )
            )
            (1): Sequential(
              (0): ResBlock(
                (in_layers): Sequential(
                  (0): GroupNorm8(8, 256, eps=1e-05, affine=True)
                  (1): SiLU()
                  (2): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                )
                (h_upd): Identity()
                (x_upd): Identity()
                (out_layers): Sequential(
                  (0): GroupNorm8(8, 128, eps=1e-05, affine=True)
                  (1): SiLU()
                  (2): Dropout(p=0, inplace=False)
                  (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                )
                (skip_connection): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
              )
              (1): AttentionBlock(
                (qkv): Conv1d(128, 384, kernel_size=(1,), stride=(1,))
                (attention): QKVAttentionLegacy()
                (norm): GroupNorm8(8, 128, eps=1e-05, affine=True)
                (proj_out): Conv1d(128, 128, kernel_size=(1,), stride=(1,))
              )
              (2): Upsample(
                (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
            )
            (2): Sequential(
              (0): ResBlock(
                (in_layers): Sequential(
                  (0): GroupNorm8(8, 256, eps=1e-05, affine=True)
                  (1): SiLU()
                  (2): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                )
                (h_upd): Identity()
                (x_upd): Identity()
                (out_layers): Sequential(
                  (0): GroupNorm8(8, 128, eps=1e-05, affine=True)
                  (1): SiLU()
                  (2): Dropout(p=0, inplace=False)
                  (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                )
                (skip_connection): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
              )
            )
            (3): Sequential(
              (0): ResBlock(
                (in_layers): Sequential(
                  (0): GroupNorm8(8, 256, eps=1e-05, affine=True)
                  (1): SiLU()
                  (2): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                )
                (h_upd): Identity()
                (x_upd): Identity()
                (out_layers): Sequential(
                  (0): GroupNorm8(8, 128, eps=1e-05, affine=True)
                  (1): SiLU()
                  (2): Dropout(p=0, inplace=False)
                  (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                )
                (skip_connection): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
              )
              (1): Upsample(
                (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
            )
            (4-5): 2 x Sequential(
              (0): ResBlock(
                (in_layers): Sequential(
                  (0): GroupNorm8(8, 256, eps=1e-05, affine=True)
                  (1): SiLU()
                  (2): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                )
                (h_upd): Identity()
                (x_upd): Identity()
                (out_layers): Sequential(
                  (0): GroupNorm8(8, 128, eps=1e-05, affine=True)
                  (1): SiLU()
                  (2): Dropout(p=0, inplace=False)
                  (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                )
                (skip_connection): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
              )
            )
          )
          (out): Sequential(
            (0): GroupNorm8(8, 128, eps=1e-05, affine=True)
            (1): SiLU()
            (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
        )
        (4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
    )
    (regressor_residual): ModuleList(
      (0): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1))
    )
    (depth_head): ModuleList(
      (0): Sequential(
        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)
        (1): GELU(approximate='none')
        (2): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)
      )
    )
    (upsampler): DPTHead(
      (concat_projects): ModuleList(
        (0): Conv2d(208, 48, kernel_size=(1, 1), stride=(1, 1))
        (1): Conv2d(353, 96, kernel_size=(1, 1), stride=(1, 1))
        (2): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (projects): ModuleList(
        (0): Conv2d(384, 48, kernel_size=(1, 1), stride=(1, 1))
        (1): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1))
        (2): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
        (3): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (resize_layers): ModuleList(
        (0): ConvTranspose2d(48, 48, kernel_size=(4, 4), stride=(4, 4))
        (1): ConvTranspose2d(96, 96, kernel_size=(2, 2), stride=(2, 2))
        (2): Identity()
        (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      )
      (scratch): Module(
        (layer1_rn): Conv2d(48, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (layer2_rn): Conv2d(96, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (layer3_rn): Conv2d(192, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (layer4_rn): Conv2d(384, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (refinenet1): FeatureFusionBlock(
          (out_conv): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))
          (resConfUnit1): ResidualConvUnit(
            (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (activation): ReLU()
            (skip_add): FloatFunctional(
              (activation_post_process): Identity()
            )
          )
          (resConfUnit2): ResidualConvUnit(
            (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (activation): ReLU()
            (skip_add): FloatFunctional(
              (activation_post_process): Identity()
            )
          )
          (skip_add): FloatFunctional(
            (activation_post_process): Identity()
          )
        )
        (refinenet2): FeatureFusionBlock(
          (out_conv): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))
          (resConfUnit1): ResidualConvUnit(
            (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (activation): ReLU()
            (skip_add): FloatFunctional(
              (activation_post_process): Identity()
            )
          )
          (resConfUnit2): ResidualConvUnit(
            (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (activation): ReLU()
            (skip_add): FloatFunctional(
              (activation_post_process): Identity()
            )
          )
          (skip_add): FloatFunctional(
            (activation_post_process): Identity()
          )
        )
        (refinenet3): FeatureFusionBlock(
          (out_conv): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))
          (resConfUnit1): ResidualConvUnit(
            (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (activation): ReLU()
            (skip_add): FloatFunctional(
              (activation_post_process): Identity()
            )
          )
          (resConfUnit2): ResidualConvUnit(
            (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (activation): ReLU()
            (skip_add): FloatFunctional(
              (activation_post_process): Identity()
            )
          )
          (skip_add): FloatFunctional(
            (activation_post_process): Identity()
          )
        )
        (refinenet4): FeatureFusionBlock(
          (out_conv): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))
          (resConfUnit2): ResidualConvUnit(
            (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (activation): ReLU()
            (skip_add): FloatFunctional(
              (activation_post_process): Identity()
            )
          )
          (skip_add): FloatFunctional(
            (activation_post_process): Identity()
          )
        )
        (output_conv): Sequential(
          (0): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)
          (1): GELU(approximate='none')
          (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)
          (3): GELU(approximate='none')
          (4): Conv2d(16, 1, kernel_size=(1, 1), stride=(1, 1))
        )
      )
    )
  )
  (feature_upsampler): DPTHead(
    (concat_projects): ModuleList(
      (0): Conv2d(208, 48, kernel_size=(1, 1), stride=(1, 1))
      (1): Conv2d(352, 96, kernel_size=(1, 1), stride=(1, 1))
      (2): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))
    )
    (projects): ModuleList(
      (0): Conv2d(384, 48, kernel_size=(1, 1), stride=(1, 1))
      (1): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1))
      (2): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      (3): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))
    )
    (resize_layers): ModuleList(
      (0): ConvTranspose2d(48, 48, kernel_size=(4, 4), stride=(4, 4))
      (1): ConvTranspose2d(96, 96, kernel_size=(2, 2), stride=(2, 2))
      (2): Identity()
      (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    )
    (scratch): Module(
      (layer1_rn): Conv2d(48, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (layer2_rn): Conv2d(96, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (layer3_rn): Conv2d(192, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (layer4_rn): Conv2d(384, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (refinenet1): FeatureFusionBlock(
        (out_conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
        (resConfUnit1): ResidualConvUnit(
          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (activation): ReLU()
          (skip_add): FloatFunctional(
            (activation_post_process): Identity()
          )
        )
        (resConfUnit2): ResidualConvUnit(
          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (activation): ReLU()
          (skip_add): FloatFunctional(
            (activation_post_process): Identity()
          )
        )
        (skip_add): FloatFunctional(
          (activation_post_process): Identity()
        )
      )
      (refinenet2): FeatureFusionBlock(
        (out_conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
        (resConfUnit1): ResidualConvUnit(
          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (activation): ReLU()
          (skip_add): FloatFunctional(
            (activation_post_process): Identity()
          )
        )
        (resConfUnit2): ResidualConvUnit(
          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (activation): ReLU()
          (skip_add): FloatFunctional(
            (activation_post_process): Identity()
          )
        )
        (skip_add): FloatFunctional(
          (activation_post_process): Identity()
        )
      )
      (refinenet3): FeatureFusionBlock(
        (out_conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
        (resConfUnit1): ResidualConvUnit(
          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (activation): ReLU()
          (skip_add): FloatFunctional(
            (activation_post_process): Identity()
          )
        )
        (resConfUnit2): ResidualConvUnit(
          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (activation): ReLU()
          (skip_add): FloatFunctional(
            (activation_post_process): Identity()
          )
        )
        (skip_add): FloatFunctional(
          (activation_post_process): Identity()
        )
      )
      (refinenet4): FeatureFusionBlock(
        (out_conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
        (resConfUnit2): ResidualConvUnit(
          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (activation): ReLU()
          (skip_add): FloatFunctional(
            (activation_post_process): Identity()
          )
        )
        (skip_add): FloatFunctional(
          (activation_post_process): Identity()
        )
      )
    )
  )
  (gaussian_adapter): GaussianAdapter()
  (pose_adjuster): PoseAdjustHead(
    (global_pool): AdaptiveAvgPool2d(output_size=1)
    (fc): Sequential(
      (0): Flatten(start_dim=1, end_dim=-1)
      (1): Linear(in_features=64, out_features=128, bias=True)
      (2): ReLU()
      (3): Linear(in_features=128, out_features=9, bias=True)
    )
  )
  (gaussian_regressor): Sequential(
    (0): Conv2d(69, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): GELU(approximate='none')
    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  )
  (gaussian_head): Sequential(
    (0): Conv2d(132, 37, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)
    (1): GELU(approximate='none')
    (2): Conv2d(37, 37, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)
  )
)
Error executing job with overrides: ['+experiment=re10k', 'data_loader.train.batch_size=1', 'dataset.test_chunk_interval=10', 'trainer.max_steps=4800000', 'model.encoder.gaussian_adapter.gaussian_scale_max=0.3', 'model.encoder.upsample_factor=4', 'model.encoder.lowest_feature_resolution=4', 'checkpointing.pretrained_model=pretrained/depthsplat-gs-small-re10k-256x256-view2-cfeab6b1.pth', 'output_dir=checkpoints/re10k-256x256-depthsplat-small', 'checkpointing.every_n_train_steps=100000', 'checkpointing.resume=False']
Traceback (most recent call last):
  File "/work/courses/3dv/35/depthsplat/src/main.py", line 241, in train
    trainer.fit(model_wrapper, datamodule=data_module, ckpt_path=checkpoint_path)
  File "/work/courses/3dv/35/envs/depthsplat/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 538, in fit
    call._call_and_handle_interrupt(
  File "/work/courses/3dv/35/envs/depthsplat/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 47, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/work/courses/3dv/35/envs/depthsplat/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 574, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/work/courses/3dv/35/envs/depthsplat/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 981, in _run
    results = self._run_stage()
  File "/work/courses/3dv/35/envs/depthsplat/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1025, in _run_stage
    self.fit_loop.run()
  File "/work/courses/3dv/35/envs/depthsplat/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py", line 205, in run
    self.advance()
  File "/work/courses/3dv/35/envs/depthsplat/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py", line 363, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/work/courses/3dv/35/envs/depthsplat/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 140, in run
    self.advance(data_fetcher)
  File "/work/courses/3dv/35/envs/depthsplat/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 250, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
  File "/work/courses/3dv/35/envs/depthsplat/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 190, in run
    self._optimizer_step(batch_idx, closure)
  File "/work/courses/3dv/35/envs/depthsplat/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 268, in _optimizer_step
    call._call_lightning_module_hook(
  File "/work/courses/3dv/35/envs/depthsplat/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 167, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/work/courses/3dv/35/envs/depthsplat/lib/python3.10/site-packages/pytorch_lightning/core/module.py", line 1306, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/work/courses/3dv/35/envs/depthsplat/lib/python3.10/site-packages/pytorch_lightning/core/optimizer.py", line 153, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
  File "/work/courses/3dv/35/envs/depthsplat/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py", line 238, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
  File "/work/courses/3dv/35/envs/depthsplat/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision.py", line 122, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "/work/courses/3dv/35/envs/depthsplat/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 130, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/work/courses/3dv/35/envs/depthsplat/lib/python3.10/site-packages/torch/optim/optimizer.py", line 484, in wrapper
    out = func(*args, **kwargs)
  File "/work/courses/3dv/35/envs/depthsplat/lib/python3.10/site-packages/torch/optim/optimizer.py", line 89, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/work/courses/3dv/35/envs/depthsplat/lib/python3.10/site-packages/torch/optim/adamw.py", line 204, in step
    loss = closure()
  File "/work/courses/3dv/35/envs/depthsplat/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision.py", line 108, in _wrap_closure
    closure_result = closure()
  File "/work/courses/3dv/35/envs/depthsplat/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 144, in __call__
    self._result = self.closure(*args, **kwargs)
  File "/work/courses/3dv/35/envs/depthsplat/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/work/courses/3dv/35/envs/depthsplat/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 129, in closure
    step_output = self._step_fn()
  File "/work/courses/3dv/35/envs/depthsplat/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 317, in _training_step
    training_step_output = call._call_strategy_hook(trainer, "training_step", *kwargs.values())
  File "/work/courses/3dv/35/envs/depthsplat/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 319, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/work/courses/3dv/35/envs/depthsplat/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py", line 390, in training_step
    return self.lightning_module.training_step(*args, **kwargs)
  File "/work/courses/3dv/35/envs/depthsplat/lib/python3.10/site-packages/jaxtyping/_decorator.py", line 522, in wrapped_fn
    return wrapped_fn_impl(args, kwargs, bound, memos)
  File "/work/courses/3dv/35/envs/depthsplat/lib/python3.10/site-packages/jaxtyping/_decorator.py", line 449, in wrapped_fn_impl
    out = fn(*args, **kwargs)
  File "/work/courses/3dv/35/depthsplat/src/model/model_wrapper.py", line 271, in training_step
    output_context = self.decoder.forward(
  File "/work/courses/3dv/35/envs/depthsplat/lib/python3.10/site-packages/jaxtyping/_decorator.py", line 522, in wrapped_fn
    return wrapped_fn_impl(args, kwargs, bound, memos)
  File "/work/courses/3dv/35/envs/depthsplat/lib/python3.10/site-packages/jaxtyping/_decorator.py", line 449, in wrapped_fn_impl
    out = fn(*args, **kwargs)
  File "/work/courses/3dv/35/depthsplat/src/model/decoder/decoder_splatting_cuda.py", line 47, in forward
    color = render_cuda(
  File "/work/courses/3dv/35/envs/depthsplat/lib/python3.10/site-packages/jaxtyping/_decorator.py", line 522, in wrapped_fn
    return wrapped_fn_impl(args, kwargs, bound, memos)
  File "/work/courses/3dv/35/envs/depthsplat/lib/python3.10/site-packages/jaxtyping/_decorator.py", line 449, in wrapped_fn_impl
    out = fn(*args, **kwargs)
  File "/work/courses/3dv/35/depthsplat/src/model/decoder/cuda_splatting.py", line 135, in render_cuda
    image, radii, depth, weights = rasterizer(
ValueError: not enough values to unpack (expected 4, got 2)

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
wandb: - 1.521 MB of 1.521 MB uploadedwandb: \ 1.566 MB of 1.566 MB uploadedwandb: 
wandb: Run history:
wandb: trainer/global_step ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: trainer/global_step 0
wandb: 
wandb: üöÄ View run re10k-256x256-depthsplat-small at: https://wandb.ai/seamoon2020-eth-z-rich/depthsplat/runs/zikte9en
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/seamoon2020-eth-z-rich/depthsplat
wandb: Synced 5 W&B file(s), 6 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: checkpoints/re10k-256x256-depthsplat-small/wandb/run-20250531_200712-zikte9en/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
